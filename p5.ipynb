{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba90c00f",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fb8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression:\n",
      "Training R^2 score: 0.613\n",
      "Testing R^2 score: 0.576\n",
      "Training MSE: 0.52\n",
      "Testing MSE: 0.56\n",
      "\n",
      "Lasso Regression:\n",
      "Training R^2 score: 0.497\n",
      "Testing R^2 score: 0.481\n",
      "Training MSE: 0.67\n",
      "Testing MSE: 0.68\n"
     ]
    }
   ],
   "source": [
    "                                #PRACTICAL 5\n",
    "\n",
    "OBJECTIVE:\n",
    "    \n",
    "Implement Linear Regression to predict house prices based on multiple variables using regularization techniques and explain how regularization overcome overfitting problem.\n",
    "\n",
    "CODE:\n",
    "    \n",
    "    \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate and train the Ridge regression model\n",
    "ridge_alpha = 0.5  # Regularization strength for Ridge\n",
    "ridge = Ridge(alpha=ridge_alpha)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Instantiate and train the Lasso regression model\n",
    "lasso_alpha = 0.1  # Regularization strength for Lasso\n",
    "lasso = Lasso(alpha=lasso_alpha)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the Ridge model\n",
    "ridge_train_score = ridge.score(X_train_scaled, y_train)\n",
    "ridge_test_score = ridge.score(X_test_scaled, y_test)\n",
    "print(\"Ridge Regression:\")\n",
    "print(f\"Training R^2 score: {ridge_train_score:.3f}\")\n",
    "print(f\"Testing R^2 score: {ridge_test_score:.3f}\")\n",
    "\n",
    "ridge_y_pred_train = ridge.predict(X_train_scaled)\n",
    "ridge_y_pred_test = ridge.predict(X_test_scaled)\n",
    "\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_y_pred_train)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_y_pred_test)\n",
    "print(f\"Training MSE: {ridge_train_mse:.2f}\")\n",
    "print(f\"Testing MSE: {ridge_test_mse:.2f}\")\n",
    "\n",
    "# Evaluate the Lasso model\n",
    "lasso_train_score = lasso.score(X_train_scaled, y_train)\n",
    "lasso_test_score = lasso.score(X_test_scaled, y_test)\n",
    "print(\"\\nLasso Regression:\")\n",
    "print(f\"Training R^2 score: {lasso_train_score:.3f}\")\n",
    "print(f\"Testing R^2 score: {lasso_test_score:.3f}\")\n",
    "\n",
    "lasso_y_pred_train = lasso.predict(X_train_scaled)\n",
    "lasso_y_pred_test = lasso.predict(X_test_scaled)\n",
    "\n",
    "lasso_train_mse = mean_squared_error(y_train, lasso_y_pred_train)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_y_pred_test)\n",
    "print(f\"Training MSE: {lasso_train_mse:.2f}\")\n",
    "print(f\"Testing MSE: {lasso_test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d18f3faa",
   "metadata": {},
   "source": [
    "Regularization, in this case, Ridge and Lasso regularization, helps overcome the overfitting problem by penalizing large coefficients during the model training process. This penalty term discourages the model from fitting the training data too closely, thereby reducing its sensitivity to noise and preventing overfitting. Consequently, the models become more robust and generalize better to unseen data, leading to improved performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562c450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f5297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
